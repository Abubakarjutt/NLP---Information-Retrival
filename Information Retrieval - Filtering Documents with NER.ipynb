{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from bert_embedding import BertEmbedding\n",
    "from operator import itemgetter\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet\n",
    "import num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rhombic ZnO nanosheets modified with Pd nanopa...</td>\n",
       "      <td>The rhombic ZnO nanosheets were prepared via a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The efficient mixed matrix antifouling membran...</td>\n",
       "      <td>Membrane technology has raised considerable in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three-dimensional carbonate reservoir geomodel...</td>\n",
       "      <td>To better know the spatial distribution and ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Development of Pr2-xSrxCuO4±δ mixed ion-electr...</td>\n",
       "      <td>Mixed ionic-electronic conducting oxides Pr2-x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comparison of methods for preparation of 125I ...</td>\n",
       "      <td>Two procedures for fixing the 125I activity on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44861</th>\n",
       "      <td>Production and optimization of high grade cell...</td>\n",
       "      <td>Production of high grade cellulolytic enzymes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44862</th>\n",
       "      <td>Feasibility of acetone–butanol–ethanol ferment...</td>\n",
       "      <td>The economic feasibility of acetone–butanol–et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44863</th>\n",
       "      <td>Index</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44864</th>\n",
       "      <td>Maximizing renewable hydrogen production from ...</td>\n",
       "      <td>Biological production of hydrogen from biomass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44865</th>\n",
       "      <td>Applying raw poultry litter leachate for the c...</td>\n",
       "      <td>In the present paper, the use of raw poultry l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      Rhombic ZnO nanosheets modified with Pd nanopa...   \n",
       "1      The efficient mixed matrix antifouling membran...   \n",
       "2      Three-dimensional carbonate reservoir geomodel...   \n",
       "3      Development of Pr2-xSrxCuO4±δ mixed ion-electr...   \n",
       "4      Comparison of methods for preparation of 125I ...   \n",
       "...                                                  ...   \n",
       "44861  Production and optimization of high grade cell...   \n",
       "44862  Feasibility of acetone–butanol–ethanol ferment...   \n",
       "44863                                              Index   \n",
       "44864  Maximizing renewable hydrogen production from ...   \n",
       "44865  Applying raw poultry litter leachate for the c...   \n",
       "\n",
       "                                                Abstract  \n",
       "0      The rhombic ZnO nanosheets were prepared via a...  \n",
       "1      Membrane technology has raised considerable in...  \n",
       "2      To better know the spatial distribution and ar...  \n",
       "3      Mixed ionic-electronic conducting oxides Pr2-x...  \n",
       "4      Two procedures for fixing the 125I activity on...  \n",
       "...                                                  ...  \n",
       "44861  Production of high grade cellulolytic enzymes ...  \n",
       "44862  The economic feasibility of acetone–butanol–et...  \n",
       "44863                                                     \n",
       "44864  Biological production of hydrogen from biomass...  \n",
       "44865  In the present paper, the use of raw poultry l...  \n",
       "\n",
       "[44866 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Title = []\n",
    "Abstract = []\n",
    "Url = []\n",
    "\n",
    "path = 'C://Users//Abubakar//Desktop//articles//'\n",
    "file_name = '_Article.json'\n",
    "\n",
    "for i in range(1, 11):\n",
    "    with open(path+str(i)+file_name, encoding=\"utf8\") as json_file:\n",
    "        temp_data = json.load(json_file)\n",
    "        for j in temp_data:\n",
    "            temp_title = j['Title']\n",
    "            temp_abstract = j['Abstract']\n",
    "            Title.append(temp_title)\n",
    "            Abstract.append(temp_abstract)\n",
    "            \n",
    "df = pd.DataFrame()\n",
    "df['Title'] = Title\n",
    "df['Abstract'] = Abstract\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    # Tokenize the input text and remove stopwords from the corpus\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 3:\n",
    "            new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    # Remove punctuations defined below from input text\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    # Remove apostrophe from the input text\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def convert_numbers(data):\n",
    "    # Convert numbers to text form in input text\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def get_bigrams(text):\n",
    "    \n",
    "    \"\"\"Input\n",
    "    ----------\n",
    "    text : str or list of strings\n",
    "    n    : number of word in each combination string ie if n = 2 the tokenization will happen in two word pairs\n",
    "    \n",
    "    Output\n",
    "    -------\n",
    "    tokens : The output would be a list of lists and each element list of the list will contain\n",
    "             unigram and n_gram tokens. This functions can be modified for a range of grams but right now\n",
    "             it will be best to use it with n = 2.\n",
    "    \"\"\"\n",
    "    text = preprocess(text)\n",
    "    bi_grams = ngrams(word_tokenize(text), 2)\n",
    "    unigrams = word_tokenize(text)\n",
    "    bigrams = [' '.join(grams) for grams in bi_grams]\n",
    "    tokens = unigrams + bigrams\n",
    "    return tokens\n",
    "\n",
    "def preprocess(data):\n",
    "    # Preprocess the input text\n",
    "    data = data.lower()\n",
    "    #data = remove_punctuation(data) #remove comma seperately\n",
    "    #data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    #data = convert_numbers(data)\n",
    "    #data = remove_punctuation(data)\n",
    "    #data = convert_numbers(data)\n",
    "    #data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    #data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data for NER Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(dataframe, column):\n",
    "    tokens = []\n",
    "    for i in dataframe[column]:\n",
    "        tokens.append(get_bigrams(i))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_tokens(df, \"Title\")\n",
    "abstracts = get_tokens(df, \"Abstract\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the Articles if there's a Geopolitical Entity\n",
    "\n",
    "We can filter articles on the basis of any type of named entity provided by SpaCy's NER API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nlp_filter_regional(dataframe, column, NER_tag):\n",
    "    '''Given data frame and column of the data frame this function outputs the indices of articles that do not have\n",
    "        the name of any goepolical entity in it.\n",
    "       \n",
    "       Input\n",
    "       -------\n",
    "       dataframe    : A pandas data frame.\n",
    "       column       : Specific column of the data frame.\n",
    "       \n",
    "       Output\n",
    "       -------\n",
    "       indices      : A list of indices of articles that does not have have any geoplolictacl entity in them.'''\n",
    "    \n",
    "    nlp = spacy.load('en_core_web_sm') # python -m spacy download en_core_web_sm\n",
    "    indices = []\n",
    "    doc_ents = []\n",
    "    trashed_docs_indices = []\n",
    "    for i in range(len(dataframe[column])):\n",
    "        document = dataframe.loc[i, column]\n",
    "        document = remove_stop_words(document)\n",
    "        document = word_tokenize(document)\n",
    "        _doc_ents = []\n",
    "        for word in document:\n",
    "            word = nlp(word)\n",
    "            for entity in word.ents:\n",
    "                _doc_ents.append(entity.label_)\n",
    "        doc_ents.append(_doc_ents)\n",
    "\n",
    "    for i in range(len(dataframe[column])):\n",
    "        doc = doc_ents[i]\n",
    "        if NER_tag not in doc:\n",
    "            indices.append(i)\n",
    "            \n",
    "        else:\n",
    "            trashed_docs_indices.append(i)\n",
    "    \n",
    "    title = []\n",
    "    abstract = []\n",
    "    for i in indices:\n",
    "        title.append(df.loc[i, 'Title'])\n",
    "        abstract.append(df.loc[i, 'Abstract'])\n",
    "    data = pd.DataFrame()\n",
    "    data['Title'] = title\n",
    "    data['Abstract'] = abstract\n",
    "    \n",
    "    \n",
    "    title = []\n",
    "    abstract = []\n",
    "    for i in trashed_docs_indices:\n",
    "        title.append(df.loc[i, 'Title'])\n",
    "        abstract.append(df.loc[i, 'Abstract'])\n",
    "    trashed_data = pd.DataFrame()\n",
    "    trashed_data['Title'] = title\n",
    "    trashed_data['Abstract'] = abstract\n",
    "    return data, trashed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_docs, trashed_docs = nlp_filter_regional(df[:100], \"Title\", \"GPE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Algae community response to climate change and nutrient loading recorded by sedimentary phytoplankton pigments in the Changtan Reservoir, China'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trashed_docs[\"Title\"][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
