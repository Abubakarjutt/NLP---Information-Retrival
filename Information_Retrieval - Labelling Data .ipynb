{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from bert_embedding import BertEmbedding\n",
    "from operator import itemgetter\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rhombic ZnO nanosheets modified with Pd nanopa...</td>\n",
       "      <td>The rhombic ZnO nanosheets were prepared via a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The efficient mixed matrix antifouling membran...</td>\n",
       "      <td>Membrane technology has raised considerable in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Three-dimensional carbonate reservoir geomodel...</td>\n",
       "      <td>To better know the spatial distribution and ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Development of Pr2-xSrxCuO4±δ mixed ion-electr...</td>\n",
       "      <td>Mixed ionic-electronic conducting oxides Pr2-x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Comparison of methods for preparation of 125I ...</td>\n",
       "      <td>Two procedures for fixing the 125I activity on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44861</th>\n",
       "      <td>Production and optimization of high grade cell...</td>\n",
       "      <td>Production of high grade cellulolytic enzymes ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44862</th>\n",
       "      <td>Feasibility of acetone–butanol–ethanol ferment...</td>\n",
       "      <td>The economic feasibility of acetone–butanol–et...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44863</th>\n",
       "      <td>Index</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44864</th>\n",
       "      <td>Maximizing renewable hydrogen production from ...</td>\n",
       "      <td>Biological production of hydrogen from biomass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44865</th>\n",
       "      <td>Applying raw poultry litter leachate for the c...</td>\n",
       "      <td>In the present paper, the use of raw poultry l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44866 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      Rhombic ZnO nanosheets modified with Pd nanopa...   \n",
       "1      The efficient mixed matrix antifouling membran...   \n",
       "2      Three-dimensional carbonate reservoir geomodel...   \n",
       "3      Development of Pr2-xSrxCuO4±δ mixed ion-electr...   \n",
       "4      Comparison of methods for preparation of 125I ...   \n",
       "...                                                  ...   \n",
       "44861  Production and optimization of high grade cell...   \n",
       "44862  Feasibility of acetone–butanol–ethanol ferment...   \n",
       "44863                                              Index   \n",
       "44864  Maximizing renewable hydrogen production from ...   \n",
       "44865  Applying raw poultry litter leachate for the c...   \n",
       "\n",
       "                                                Abstract  \n",
       "0      The rhombic ZnO nanosheets were prepared via a...  \n",
       "1      Membrane technology has raised considerable in...  \n",
       "2      To better know the spatial distribution and ar...  \n",
       "3      Mixed ionic-electronic conducting oxides Pr2-x...  \n",
       "4      Two procedures for fixing the 125I activity on...  \n",
       "...                                                  ...  \n",
       "44861  Production of high grade cellulolytic enzymes ...  \n",
       "44862  The economic feasibility of acetone–butanol–et...  \n",
       "44863                                                     \n",
       "44864  Biological production of hydrogen from biomass...  \n",
       "44865  In the present paper, the use of raw poultry l...  \n",
       "\n",
       "[44866 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Title = []\n",
    "Abstract = []\n",
    "Url = []\n",
    "\n",
    "path = 'C://Users//Abubakar//Desktop//articles//'\n",
    "file_name = '_Article.json'\n",
    "\n",
    "for i in range(1, 11):\n",
    "    with open(path+str(i)+file_name, encoding=\"utf8\") as json_file:\n",
    "        temp_data = json.load(json_file)\n",
    "        for j in temp_data:\n",
    "            temp_title = j['Title']\n",
    "            temp_abstract = j['Abstract']\n",
    "            Title.append(temp_title)\n",
    "            Abstract.append(temp_abstract)\n",
    "            \n",
    "df = pd.DataFrame()\n",
    "df['Title'] = Title\n",
    "df['Abstract'] = Abstract\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Data Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    # Tokenize the input text and remove stopwords from the corpus\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 3:\n",
    "            new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    # Remove punctuations defined below from input text\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    # Remove apostrophe from the input text\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def convert_numbers(data):\n",
    "    # Convert numbers to text form in input text\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def get_bigrams(text):\n",
    "    \n",
    "    \"\"\"Input\n",
    "    ----------\n",
    "    text : str or list of strings\n",
    "    n    : number of word in each combination string ie if n = 2 the tokenization will happen in two word pairs\n",
    "    \n",
    "    Output\n",
    "    -------\n",
    "    tokens : The output would be a list of lists and each element list of the list will contain\n",
    "             unigram and n_gram tokens. This functions can be modified for a range of grams but right now\n",
    "             it will be best to use it with n = 2.\n",
    "    \"\"\"\n",
    "    text = preprocess(text)\n",
    "    bi_grams = ngrams(word_tokenize(text), 2)\n",
    "    unigrams = word_tokenize(text)\n",
    "    bigrams = [' '.join(grams) for grams in bi_grams]\n",
    "    tokens = unigrams + bigrams\n",
    "    return tokens\n",
    "\n",
    "def preprocess(data):\n",
    "    # Preprocess the input text\n",
    "    data = data.lower()\n",
    "    #data = remove_punctuation(data) #remove comma seperately\n",
    "    #data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    #data = convert_numbers(data)\n",
    "    #data = remove_punctuation(data)\n",
    "    #data = convert_numbers(data)\n",
    "    #data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    #data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(dataframe, column):\n",
    "    tokens = []\n",
    "    for i in dataframe[column]:\n",
    "        tokens.append(get_bigrams(i))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define TFIDF Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_scores(abstract, title):\n",
    "    \n",
    "    \"\"\"given body and titles of the articles this funtion calculates the tfidf scores of the words in the text corpus.\n",
    "       \n",
    "       Input\n",
    "       -------\n",
    "       body         : Body or abstracts of the articles.\n",
    "       title        : Titles of the articles\n",
    "       \n",
    "       Output\n",
    "       -------\n",
    "       tf_idf       : A dictionary of tf_idf scorese of the vocabulary.\n",
    "       \"\"\"\n",
    "   \n",
    "    \n",
    "    N = len(abstract)\n",
    "    DF = {}\n",
    "\n",
    "    for i in range(N):\n",
    "        tokens = abstract[i]\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                DF[w].add(i)\n",
    "            except:\n",
    "                DF[w] = {i}\n",
    "    for i in DF:\n",
    "        DF[i] = len(DF[i])\n",
    "    \n",
    "    total_vocab = [x for x in DF]\n",
    "\n",
    "    doc = 0\n",
    "\n",
    "    tf_idf = {}\n",
    "\n",
    "    for i in range(N):\n",
    "    \n",
    "        tokens = abstract[i]\n",
    "    \n",
    "        counter = Counter(tokens + title[i])\n",
    "        words_count = len(tokens + title[i])\n",
    "    \n",
    "        for token in np.unique(tokens):\n",
    "        \n",
    "            tf = counter[token]/words_count\n",
    "            df = doc_freq(token)\n",
    "            idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "            tf_idf[doc, token] = tf*idf\n",
    "\n",
    "        doc += 1\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_abstract_title_filter(query, abstract, title):\n",
    "    \"\"\"given the query, body of articles, titles of articles and tfifd_method\n",
    "      this funtion extract k number of articles that are most relevent to the query keywords.\n",
    "       \n",
    "       Input\n",
    "       -------\n",
    "       body         : Body or abstracts of the articles.\n",
    "       title        : Titles of the articles\n",
    "       \n",
    "       Output\n",
    "       -------\n",
    "       tf_idf       : A dictionary of tf_idf scorese of the vocabulary.\"\"\"\n",
    "    \n",
    "    \n",
    "    tokens = get_bigrams(query)\n",
    "    \n",
    "    tf_idf = tfidf_scores(abstract, title)\n",
    "    \n",
    "    relevent_indices = []\n",
    "    \n",
    "    query_weights = {}\n",
    "\n",
    "    for key in tf_idf:\n",
    "        \n",
    "        if key[1] in tokens:\n",
    "            try:\n",
    "                query_weights[key[0]] += tf_idf[key]\n",
    "            except:\n",
    "                query_weights[key[0]] = tf_idf[key]\n",
    "    \n",
    "    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)    \n",
    "    \n",
    "    for i in query_weights:\n",
    "        relevent_indices.append(i[0])\n",
    "\n",
    "        \n",
    "    return relevent_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply K Means Clustering to Get Keywords for Categories\n",
    "\n",
    "By applying the K Means Clustering we can get the keywords that are most common in each cluster, later we can use these keywords to assign a class to each cluster and use these classes to label the unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = get_tokens(df, \"Title\")\n",
    "abstracts = get_tokens(df, \"Abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=3, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Abstract'])\n",
    "\n",
    "n_clusters = 3\n",
    "model = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "              lipid\n",
      "              biomass\n",
      "              microalgae\n",
      "Cluster 1:\n",
      "              production\n",
      "              energy\n",
      "              biomass\n",
      "Cluster 2:\n",
      "              algae\n",
      "              water\n",
      "              production\n"
     ]
    }
   ],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(n_clusters):\n",
    " print('Cluster %d:' % i),\n",
    " for ind in order_centroids[i, 0:3]:\n",
    "        print('              %s' % terms[ind])\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Labeling Using Keywords from Cluster and TFIDF Similarity\n",
    "\n",
    "Using the keywords in the clusters we can define categories and use these categories as queries in TFIDF Algorithm, the articles that are most similar to a certain category would be assigned that category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clusters = ['Algae production in Water', 'Production of Biomass Energy', 'Liquid Biomass and Micro Algea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abubakar\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a2ae4f87644dc891e775cc521424bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The potential of optimized process design to a...</td>\n",
       "      <td>Environmental impact is an essential aspect fo...</td>\n",
       "      <td>Algae production in Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The potential of optimized process design to a...</td>\n",
       "      <td>Environmental impact is an essential aspect fo...</td>\n",
       "      <td>Algae production in Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The potential of optimized process design to a...</td>\n",
       "      <td>Environmental impact is an essential aspect fo...</td>\n",
       "      <td>Algae production in Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sterols from green and blue-green algae grown ...</td>\n",
       "      <td>Two green algae,Scenedesmus sp. andChlorella v...</td>\n",
       "      <td>Algae production in Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sterols from green and blue-green algae grown ...</td>\n",
       "      <td>Two green algae,Scenedesmus sp. andChlorella v...</td>\n",
       "      <td>Algae production in Water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33808</th>\n",
       "      <td>Chapter 1: Advancement of Metabolomics Techniq...</td>\n",
       "      <td>Metabolomics is the study of the whole metabol...</td>\n",
       "      <td>Liquid Biomass and Micro Algea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33809</th>\n",
       "      <td>Development of a simple and efficient method o...</td>\n",
       "      <td>Chemical isotope labeling (CIL) liquid chromat...</td>\n",
       "      <td>Liquid Biomass and Micro Algea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33810</th>\n",
       "      <td>Production of biofuels by thermal catalytic cr...</td>\n",
       "      <td>In this work, the residual fat material (scum)...</td>\n",
       "      <td>Liquid Biomass and Micro Algea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33811</th>\n",
       "      <td>State-of-the-art on detoxification of Jatropha...</td>\n",
       "      <td>Jatropha curcas seeds contain 250–300 g oil/kg...</td>\n",
       "      <td>Liquid Biomass and Micro Algea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33812</th>\n",
       "      <td>Removal of B. cereus cereulide toxin from mono...</td>\n",
       "      <td>A rapid and sensitive liquid chromatography-ma...</td>\n",
       "      <td>Liquid Biomass and Micro Algea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33813 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "0      The potential of optimized process design to a...   \n",
       "1      The potential of optimized process design to a...   \n",
       "2      The potential of optimized process design to a...   \n",
       "3      Sterols from green and blue-green algae grown ...   \n",
       "4      Sterols from green and blue-green algae grown ...   \n",
       "...                                                  ...   \n",
       "33808  Chapter 1: Advancement of Metabolomics Techniq...   \n",
       "33809  Development of a simple and efficient method o...   \n",
       "33810  Production of biofuels by thermal catalytic cr...   \n",
       "33811  State-of-the-art on detoxification of Jatropha...   \n",
       "33812  Removal of B. cereus cereulide toxin from mono...   \n",
       "\n",
       "                                                Abstract  \\\n",
       "0      Environmental impact is an essential aspect fo...   \n",
       "1      Environmental impact is an essential aspect fo...   \n",
       "2      Environmental impact is an essential aspect fo...   \n",
       "3      Two green algae,Scenedesmus sp. andChlorella v...   \n",
       "4      Two green algae,Scenedesmus sp. andChlorella v...   \n",
       "...                                                  ...   \n",
       "33808  Metabolomics is the study of the whole metabol...   \n",
       "33809  Chemical isotope labeling (CIL) liquid chromat...   \n",
       "33810  In this work, the residual fat material (scum)...   \n",
       "33811  Jatropha curcas seeds contain 250–300 g oil/kg...   \n",
       "33812  A rapid and sensitive liquid chromatography-ma...   \n",
       "\n",
       "                             Category  \n",
       "0           Algae production in Water  \n",
       "1           Algae production in Water  \n",
       "2           Algae production in Water  \n",
       "3           Algae production in Water  \n",
       "4           Algae production in Water  \n",
       "...                               ...  \n",
       "33808  Liquid Biomass and Micro Algea  \n",
       "33809  Liquid Biomass and Micro Algea  \n",
       "33810  Liquid Biomass and Micro Algea  \n",
       "33811  Liquid Biomass and Micro Algea  \n",
       "33812  Liquid Biomass and Micro Algea  \n",
       "\n",
       "[33813 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "def get_labels_and_indices(n_cluster, centroid_keywords):\n",
    "    relevent_indices = []\n",
    "    labels = []\n",
    "    for i in tqdm_notebook(range(len(clusters))):\n",
    "        for cluster in clusters:\n",
    "            query = cluster\n",
    "            _relevent_indices = tfidf_abstract_title_filter(query, abstracts, titles)\n",
    "            for i in _relevent_indices:\n",
    "                if i not in relevent_indices:\n",
    "                    relevent_indices.append(i)\n",
    "                    labels.append(query)\n",
    "    title = []\n",
    "    abstract = []\n",
    "    for i in relevent_indices:\n",
    "        title.append(df.loc[i, 'Title'])\n",
    "        abstract.append(df.loc[i, 'Abstract'])\n",
    "    data = pd.DataFrame()\n",
    "    data['Title'] = title\n",
    "    data['Abstract'] = abstract\n",
    "    data['Category'] = labels \n",
    "    return data \n",
    "\n",
    "labeled_data = get_labels_and_indices(3, order_centroids)\n",
    "labeled_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
