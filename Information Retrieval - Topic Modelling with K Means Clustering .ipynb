{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from bert_embedding import BertEmbedding\n",
    "from operator import itemgetter\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import spacy\n",
    "import gensim.downloader as api\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    # Tokenize the input text and remove stopwords from the corpus\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 3:\n",
    "            new_text = new_text + \" \" + lemmatizer.lemmatize(w)\n",
    "    return new_text\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    # Remove punctuations defined below from input text\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    # Remove apostrophe from the input text\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "def convert_numbers(data):\n",
    "    # Convert numbers to text form in input text\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def get_bigrams(text):\n",
    "    \n",
    "    \"\"\"Input\n",
    "    ----------\n",
    "    text : str or list of strings\n",
    "    n    : number of word in each combination string ie if n = 2 the tokenization will happen in two word pairs\n",
    "    \n",
    "    Output\n",
    "    -------\n",
    "    tokens : The output would be a list of lists and each element list of the list will contain\n",
    "             unigram and n_gram tokens. This functions can be modified for a range of grams but right now\n",
    "             it will be best to use it with n = 2.\n",
    "    \"\"\"\n",
    "    text = preprocess(text)\n",
    "    bi_grams = ngrams(word_tokenize(text), 2)\n",
    "    unigrams = word_tokenize(text)\n",
    "    bigrams = [' '.join(grams) for grams in bi_grams]\n",
    "    tokens = unigrams + bigrams\n",
    "    return tokens\n",
    "\n",
    "def preprocess(data):\n",
    "    # Preprocess the input text\n",
    "    data = data.lower()\n",
    "    #data = remove_punctuation(data) #remove comma seperately\n",
    "    #data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    #data = convert_numbers(data)\n",
    "    #data = remove_punctuation(data)\n",
    "    #data = convert_numbers(data)\n",
    "    #data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    #data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract = []\n",
    "title = []\n",
    "import json\n",
    "location = 'C://Users//Abubakar//Desktop//articles//'\n",
    "file_name = '_Article.json'\n",
    "\n",
    "for i in range(1, 11):\n",
    "    with open(location+str(i)+file_name, encoding=\"utf8\") as json_file:\n",
    "        temp_data = json.load(json_file)\n",
    "        for j in temp_data:\n",
    "            temp_title = j['Title']\n",
    "            temp_abstract = j['Abstract']\n",
    "            title.append(preprocess(temp_title))\n",
    "            abstract.append(preprocess(temp_abstract))\n",
    "df = pd.DataFrame(title, columns = ['Title'])\n",
    "df['Abstract'] = abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K Mean Clustering for Topic Modeling\n",
    "\n",
    "Using these keywords and using IFIDF ranking algorithm we can label the unlabed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=10, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(df['Abstract'])\n",
    "\n",
    "n_clusters = 10\n",
    "model = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "              biodiesel\n",
      "              transesterification\n",
      "              production\n",
      "Cluster 1:\n",
      "              lipid\n",
      "              acid\n",
      "              fatty\n",
      "Cluster 2:\n",
      "              ethanol\n",
      "              fermentation\n",
      "              production\n",
      "Cluster 3:\n",
      "              microalgae\n",
      "              wastewater\n",
      "              biomass\n",
      "Cluster 4:\n",
      "              algae\n",
      "              concentration\n",
      "              water\n",
      "Cluster 5:\n",
      "              energy\n",
      "              fuel\n",
      "              biofuels\n",
      "Cluster 6:\n",
      "              hydrogen\n",
      "              methane\n",
      "              biogas\n",
      "Cluster 7:\n",
      "              process\n",
      "              product\n",
      "              application\n",
      "Cluster 8:\n",
      "              bio\n",
      "              pyrolysis\n",
      "              oil\n",
      "Cluster 9:\n",
      "              membrane\n",
      "              fouling\n",
      "              flux\n"
     ]
    }
   ],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    " print('Cluster %d:' % i),\n",
    " for ind in order_centroids[i, 0:3]:\n",
    "        print('              %s' % terms[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
